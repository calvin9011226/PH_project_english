"""
import pandas as pd
import datetime as dt
import googlemaps
from populartimes import get_id

GOOGLE_API_KEY = "AIzaSyBkBeV2pKxDvLzQmcCe1X6jkqWMFhVXiuI"

def build_history_crowd_table(history_csv_file):
    df = pd.read_csv(history_csv_file)
    df['Time'] = pd.to_datetime(df['Time'], format='%m/%d/%Y %I:%M:%S %p', errors='coerce')
    df = df.dropna(subset=['Time'])
    df['Hour'] = df['Time'].dt.hour
    history_crowd = df.groupby(['è¨­ç½®é»', 'Hour']).size().reset_index(name='history_count')
    return history_crowd

def build_populartimes_table(place_list, gmaps):
    rows = []
    for place in place_list:
        try:
            res = gmaps.find_place(input=place, input_type="textquery", fields=["place_id"])
            if res.get("candidates"):
                place_id = res["candidates"][0]["place_id"]
                pop_data = get_id(GOOGLE_API_KEY, place_id)
                for day_data in pop_data.get("populartimes", []):
                    for hour, value in enumerate(day_data.get("data", [])):
                        rows.append({"è¨­ç½®é»": place, "Hour": hour, "populartimes_count": value})
        except Exception as e:
            print(f"âŒ {place} ç„¡æ³•æŠ“å– populartimesï¼š{e}")
        print("build_populartimes_table å®Œæˆåº¦")
    return pd.DataFrame(rows)

def analyze_correlation(history_csv_file):
    print("\nğŸ“Š é–‹å§‹è¨ˆç®—æ­·å²äººæ½®èˆ‡ populartimes çš„æ¯å°æ™‚ç›¸é—œä¿‚æ•¸\n")
    gmaps = googlemaps.Client(key=GOOGLE_API_KEY)

    # æ­¥é©Ÿä¸€ï¼šæ­·å²äººæ½®çµ±è¨ˆ
    history_table = build_history_crowd_table(history_csv_file)
    print("\n å–å¾—æ­·å²äººæ½®è³‡æ–™")

    # æ­¥é©ŸäºŒï¼šæŠ“å– populartimes è³‡æ–™
    place_list = history_table['è¨­ç½®é»'].unique()
    populartimes_table = build_populartimes_table(place_list, gmaps)
    print("\n å–å¾— populartimes è³‡æ–™")

    # æ­¥é©Ÿä¸‰ï¼šåˆä½µå…©ä»½è³‡æ–™
    merged = pd.merge(history_table, populartimes_table, on=['è¨­ç½®é»', 'Hour'])
    print("\n åˆä½µ populartimes å’Œ æ­·å²äººæ½®è³‡æ–™")

    # æ­¥é©Ÿå››ï¼šä¾æ“šæ¯å€‹åœ°é»è¨ˆç®— Pearson correlation
    result = []
    for place in merged['è¨­ç½®é»'].unique():
        sub = merged[merged['è¨­ç½®é»'] == place]
        if len(sub) >= 5:
            corr = sub['history_count'].corr(sub['populartimes_count'])
            result.append({"è¨­ç½®é»": place, "ç›¸é—œä¿‚æ•¸": round(corr, 3), "è³‡æ–™ç­†æ•¸": len(sub)})
        else:
            result.append({"è¨­ç½®é»": place, "ç›¸é—œä¿‚æ•¸": None, "è³‡æ–™ç­†æ•¸": len(sub)})
    print("\nè¨ˆç®—ç›¸é—œä¿‚æ•¸")

    # è¼¸å‡ºåˆ†æçµæœ
    result_df = pd.DataFrame(result)
    print(result_df.sort_values(by='ç›¸é—œä¿‚æ•¸', ascending=False).to_string(index=False))
    return result_df

# ä½¿ç”¨ç¯„ä¾‹ï¼š
analyze_correlation("./penghu_csv_file/Beacon20220907-crowd.csv")
"""


import pandas as pd
import datetime as dt
import googlemaps
from populartimes import get_id
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np
import os
import matplotlib.pyplot as plt

#è®“pltå¯ä»¥é¡¯ç¤ºä¸­æ–‡
plt.rcParams["font.family"] = "DFKai-SB"  
plt.rcParams["axes.unicode_minus"] = False 

GOOGLE_API_KEY = "AIzaSyB25tMdD2SXv7VWi7JMT5DD0sR1ON9WCw4"

def build_history_crowd_table(history_csv_file):
    df = pd.read_csv(history_csv_file)
    df['Time'] = pd.to_datetime(df['Time'], format='%m/%d/%Y %I:%M:%S %p', errors='coerce')
    df = df.dropna(subset=['Time'])
    df['Hour'] = df['Time'].dt.hour
    history_crowd = df.groupby(['è¨­ç½®é»', 'Hour']).size().reset_index(name='history_count')
    return history_crowd

def build_populartimes_table(place_list, gmaps, save_path=None,input_path=None):
    if input_path is None:      
        rows = []
        i=0
        for place in place_list:
            try:
                res = gmaps.find_place(input=place, input_type="textquery", fields=["place_id"])
                if res.get("candidates"):
                    place_id = res["candidates"][0]["place_id"]
                    pop_data = get_id(GOOGLE_API_KEY, place_id)
                    yesterday_idx = (dt.datetime.today().weekday() - 1) % 7
                    
                    for j, day_data in enumerate(pop_data.get("populartimes", [])):
                        if j == yesterday_idx:
                            for hour, value in enumerate(day_data.get("data", [])):
                                rows.append({"è¨­ç½®é»": place, "Hour": hour, "populartimes_count": value})
                    i += 1
                print(f"populartimeså®Œæˆåº¦:{round(i/len(place_list)*100,3)}%")
            except Exception as e:
                    print(f"âŒ {place} ç„¡æ³•æŠ“å– populartimesï¼š{e}")


        df = pd.DataFrame(rows)
        
        # âœ… å¦‚æœçµ¦å®šå„²å­˜è·¯å¾‘ï¼Œå°±æŠŠçµæœå¯«å‡ºä¾†
        if save_path:
            df.to_csv(save_path, index=False, encoding='utf-8-sig')
            print(f"âœ… populartimes è³‡æ–™å·²å„²å­˜è‡³ï¼š{save_path}")
    else:
        df = pd.read_csv(input_path)
    
    return df

def analyze_correlation(history_csv_file,intime_csv_file=None,result_csv_file=None,yesterday="2025"):
    print("\nğŸ“Š é–‹å§‹è¨ˆç®—æ­·å²äººæ½®èˆ‡ populartimes çš„æ¯å°æ™‚ç›¸é—œä¿‚æ•¸èˆ‡èª¤å·®æŒ‡æ¨™\n")
    gmaps = googlemaps.Client(key=GOOGLE_API_KEY)

    # æ­¥é©Ÿä¸€ï¼šæ­·å²äººæ½®çµ±è¨ˆ
    history_table = build_history_crowd_table(history_csv_file)
    print("\n âœ… å–å¾—æ­·å²äººæ½®è³‡æ–™")

    # æ­¥é©ŸäºŒï¼šæŠ“å– populartimes è³‡æ–™
    place_list = history_table['è¨­ç½®é»'].unique()
    populartimes_table = build_populartimes_table(place_list, gmaps,intime_csv_file)
    print("\n âœ…  å–å¾— populartimes è³‡æ–™")

    # æ­¥é©Ÿä¸‰ï¼šåˆä½µå…©ä»½è³‡æ–™
    merged = pd.merge(history_table, populartimes_table, on=['è¨­ç½®é»', 'Hour'])
    print("\n âœ…  åˆä½µ populartimes å’Œ æ­·å²äººæ½®è³‡æ–™")


    # æ­¥é©Ÿå››ï¼šä¾æ“šæ¯å€‹åœ°é»è¨ˆç®— Pearson correlationã€MAEã€MSE
    result = []
    #print(merged['è¨­ç½®é»'].unique())
    for place in merged['è¨­ç½®é»'].unique():
        sub = merged[merged['è¨­ç½®é»'] == place]
        #print(sub)
        if len(sub) >= 5:
            total_history = sub['history_count'].sum()
            total_popular = sub['populartimes_count'].sum()
            #print("history_count:",sub['history_count'])
            #print("populartimes_count:",sub['populartimes_count'])
            corr = sub['history_count'].corr(sub['populartimes_count'])
            #print(corr)
            mae = mean_absolute_error(sub['history_count'], sub['populartimes_count'])
            mse = mean_squared_error(sub['history_count'], sub['populartimes_count'])

            result.append({
                "è¨­ç½®é»": place,
                "ç›¸é—œä¿‚æ•¸": round(corr, 3),
                "MAE": round(mae, 2),
                "MSE": round(mse, 2),
                "æ­·å²äººæ½®ç¸½æ•¸": total_history,
                "populartimes ç¸½æ•¸": total_popular,
                "è³‡æ–™ç­†æ•¸": len(sub)
            })
        else:
            result.append({
                "è¨­ç½®é»": place,
                "ç›¸é—œä¿‚æ•¸": None,
                "MAE": None,
                "MSE": None,
                "æ­·å²äººæ½®ç¸½æ•¸": total_history,
                "populartimes ç¸½æ•¸": total_popular,
                "è³‡æ–™ç­†æ•¸": len(sub)
            })

    result_df = pd.DataFrame(result)
    #print(result_df.sort_values(by='ç›¸é—œä¿‚æ•¸', ascending=False).to_string(index=False))
    result_df.to_csv(result_csv_file, index=False, encoding='utf-8-sig')
    print("âœ… åœ°é»åˆ†æçµæœå·²å„²å­˜è‡³ correlation_result.csv")

    # é¡å¤–ï¼šç•«å‡ºæ¯å°æ™‚çš„ç›¸é—œä¿‚æ•¸æŠ˜ç·šåœ–ï¼ˆè·¨æ™¯é»ï¼‰
    hourly_corrs = []
    hourly_mae = []
    for hour in range(24):
        sub = merged[merged['Hour'] == hour]
        if len(sub) >= 1:
            corr = sub['history_count'].corr(sub['populartimes_count'])
            mae = mean_absolute_error(sub['history_count'], sub['populartimes_count'])
            mse = mean_squared_error(sub['history_count'], sub['populartimes_count'])
            hourly_corrs.append(corr)
            hourly_mae.append(mae)
        else:
            hourly_corrs.append(np.nan)
            hourly_mae.append(np.nan)
    
    plt.figure(figsize=(10, 5))
    plt.plot(range(24), hourly_mae, marker='x', linestyle='-', linewidth=2)
    plt.title(f"æ¯å°æ™‚æ­·å²äººæ½®èˆ‡åŠæ™‚äººæ½®çš„ MAE\n{yesterday}", fontsize=24)
    plt.xlabel("å°æ™‚ (Hour)", fontsize=16)
    plt.ylabel("MAE", fontsize=16)
    plt.xticks(range(24))
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(f"./penghu_csv_file/histroy_intime/intime-crowd{yesterday}.png")

    plt.figure(figsize=(10, 5))
    plt.plot(range(24), hourly_corrs, marker='o', linestyle='-', linewidth=2)
    plt.title(f"æ¯å°æ™‚æ­·å²äººæ½®èˆ‡åŠæ™‚äººæ½®çš„ç›¸é—œä¿‚æ•¸\n{yesterday}", fontsize=24)
    plt.ylabel("ç›¸é—œä¿‚æ•¸", fontsize=16)
    plt.xticks(range(24))
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(f"./penghu_csv_file/histroy_intime/result_ç›¸é—œä¿‚æ•¸{yesterday}.png")

    return result_df

# ä½¿ç”¨ç¯„ä¾‹ï¼š

yesterday = (dt.datetime.today() - dt.timedelta(days=1)).strftime("%Y-%m-%d")
analyze_correlation("./penghu_csv_file/Beacon20220907-crowd.csv",f"./penghu_csv_file/histroy_intime/intime-crowd{yesterday}.csv",f"./penghu_csv_file/histroy_intime/result_ç›¸é—œä¿‚æ•¸{yesterday}.csv",yesterday)
